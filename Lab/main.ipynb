{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression.py\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class LinearRegression: \n",
    "    def __init__(self, data_set, response):\n",
    "        self.data_set = data_set \n",
    "        self.response = response \n",
    "    \n",
    "    @property \n",
    "    def X(self):\n",
    "        X = self.data_set.drop(columns=[self.response])\n",
    "        X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "        return X\n",
    "    \n",
    "    @property\n",
    "    def Y(self):\n",
    "        Y = self.data_set[self.response]\n",
    "        Y = np.array(Y)\n",
    "        return Y\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    @property\n",
    "    def d(self):\n",
    "        return self.X.shape[1]-1\n",
    "    \n",
    "    @property\n",
    "    def column_names(self):\n",
    "        features = list(self.data_set.drop(columns=[self.response]).columns)\n",
    "        column_names = { \"X\":features, \"Y\":[self.response], }\n",
    "        return column_names\n",
    "    \n",
    "    # Ordinary Least Square \n",
    "    @property \n",
    "    def b(self):\n",
    "        X = self.X \n",
    "        Y = self.Y\n",
    "        return np.linalg.pinv( X.T @ X) @ X.T @ Y\n",
    "        \n",
    "    # SSE | Residual Sum of Squares | Sum of Squared Error\n",
    "    # The closer this is to zero the more accuret the prediction (in theory)\n",
    "    @property \n",
    "    def SSE(self):\n",
    "        X, Y, b = self.X, self.Y, self.b\n",
    "        return  sum( np.square( Y - X @ b ) )\n",
    "    \n",
    "    # SSR | Explained Sum of Squares | Regression Sum of Squares\n",
    "    @property\n",
    "    def SSR(self):\n",
    "        X, Y_mean, b = self.X, self.Y.mean(), self.b\n",
    "        return sum( np.square( X @ b - Y_mean ) )\n",
    "    \n",
    "    # SST | Syy | Total Sum of Squares\n",
    "    @property \n",
    "    def SST(self):\n",
    "        Y = self.Y\n",
    "        Y_mean = Y.mean() \n",
    "        return sum( np.square(Y-Y_mean) )\n",
    "\n",
    "    def print_all(self):\n",
    "        all = f\"\"\"\n",
    "G\n",
    "1. Number of Features\n",
    "   {self.d}\n",
    "\n",
    "2. Sample Size\n",
    "   {self.n}\n",
    "\n",
    "3. Variance  \n",
    "   {self.var()}\n",
    "\n",
    "4. Standard Deviation \n",
    "    {self.std()}\n",
    "\n",
    "5. Significance of the Regression \n",
    "    {self.sig().split(\"\\n\")[0]}\n",
    "\n",
    "6. Relevance of the Regression\n",
    "    {self.rel()}\n",
    "\n",
    "VG\n",
    "1. Significance tests on Individual Variables\n",
    "    { str().join( f\"{row}\\n{str():<6}\" for row in self.sig_var().split(\"\\n\") ) }\n",
    "\n",
    "2. A function or method that calculates the Pearson number\n",
    "    { str().join( f\"{row}\\n{str():<4}\" for row in self.r().split(\"\\n\") ) }\n",
    "         \"\"\"\n",
    "        print(all)\n",
    "\n",
    "    # The Method that Calculates The Variance \n",
    "    # sigma^2 == SEE divded by the Degrees of Freedom\n",
    "    # On average how far our alculated responses are from the regression line\n",
    "    def var(self):\n",
    "        SSE, n, d = self.SSE, self.n, self.d \n",
    "        return SSE/(n - d - 1)\n",
    "\n",
    "    \n",
    "    # The Method that Calculated The Standard Deviation | S | sigma\n",
    "    # Meassure the same thing as the Variances but in a smaller unit\n",
    "    def std(self):\n",
    "        var = self.var()\n",
    "        return np.sqrt(var)\n",
    "    \n",
    "    # The Method that reports The Significance of the Regression\n",
    "    # The closer the pvalue is to zero the more \n",
    "    # confidently we can reject the null-hypothesis (H0)\n",
    "    # If the null-hypothesis is true then it means that \n",
    "    # there is no relationship between the features and the response\n",
    "    # In order to reject the H0-hypothesis we want the p-value of at least 0.05 aka 5%\n",
    "    # this would mean that there is a 95% chances that our features effect the response\n",
    "    def sig(self): \n",
    "        SSR, d, n, std = self.SSR, self.d, self.n, self.std()\n",
    "        \n",
    "        # This is probably and inbetween value, it doesn't mean anything\n",
    "        # it shouldn't be part of the printout\n",
    "        sig_statistic = (SSR/d)/std \n",
    "     \n",
    "        # Survival Function of the F-Distrubution\n",
    "        p_significance = stats.f.sf(sig_statistic, d, n-d-1)\n",
    "        \n",
    "        # Pearsons deal with the T-Distrubution\n",
    "\n",
    "        #X, Y, b = self.X, self.Y, self.b\n",
    "        #pearson = stats.pearsonr(np.square(X @ b),Y)\n",
    "        #return f\"statistic:{sig_statistic}, pvalue:{p_significance}\\n{pearson}\"\n",
    "        result = str() \n",
    "        if p_significance < 0.0001: \n",
    "            result = \"The p-value low enough to confidently reject the null-hypothesis\"\n",
    "        elif p_significance < 0.001: \n",
    "            result = \"The p-value low enough to, with some confidence reject the null-hypothesis \"    \n",
    "        elif p_significance < 0.05: \n",
    "            result = \"The p-value is just low enough to reject the null-hypothesis\"\n",
    "        else:\n",
    "            result = \"The p-value is to high to confidetly reject the null-hypothesis\"\n",
    "\n",
    "        return f\"pvalue: {p_significance}\\n{result}\"\n",
    "    \n",
    "    # The method that reports The Relevance of Regression | R2 \n",
    "    # Reports how big percent of the calculated responses that falls withn \n",
    "    # our normal distribution aka how big of a range of responses that our model can \n",
    "    # reliably predict. So if our R2 value is 0.90 than that means that of the test\n",
    "    # cases the normal distribution of our model covers 90% of the responses \n",
    "    def rel(self):\n",
    "        SSR, SST = self.SSR, self.SST\n",
    "        return SSR/SST\n",
    "\n",
    "\n",
    "    # VG \n",
    "\n",
    "    # Significance of the Variables(Coefficients? + Bias?)\n",
    "    def sig_var(self):   \n",
    "        X, b, d, n, std, var, X_names = self.X, self.b, self.d, self.n, self.std(), self.var(), self.column_names[\"X\"]\n",
    "        X_names.insert(0,\"Bias\")\n",
    "\n",
    "        # Variance/Covariance Matrix\n",
    "        c = np.linalg.pinv( (X.T @ X) )*var\n",
    "\n",
    "        # It doesn't matter that we get nan values because \n",
    "        # where only intrested in the center values aka c[i,i] values \n",
    "        # Significans Statisitca Array(?)\n",
    "        ssa = [ b[i]/(std * np.sqrt(c[i,i])) for i in range(c.shape[1])]\n",
    "        cdf = stats.t.cdf(ssa, n-d-1)\n",
    "        sf =  stats.t.sf(ssa, n-d-1)\n",
    "        p = [ 2 * min(cdf[idx], sf[idx]) for idx in range(len(ssa)) ]\n",
    "        result = str().join( f\"p-value {X_names[idx]:<10}: {p[idx]}\\n\" for idx in range(len(p))  )\n",
    "        return result\n",
    "\n",
    "    # The Method that calculates the Pearson number between all pairs of parameters\n",
    "    def r(self):\n",
    "        X, Y, column_names = self.X, self.Y, self.column_names \n",
    "        \n",
    "        result = list()\n",
    "        \n",
    "        # Include Y (change X to XY)\n",
    "        # Remvoes the bias, add the response\n",
    "        #XY = np.column_stack( [X[:,1:], Y])\n",
    "        #names = [*column_names[\"X\"], *column_names[\"Y\"]]\n",
    "\n",
    "        #Remvoes the bias\n",
    "        X = X[:,1:]\n",
    "        names = column_names[\"X\"]\n",
    "\n",
    "  \n",
    "        for idx in range(len(names)):\n",
    "            for idy in range(idx):\n",
    "                if idy == idx:\n",
    "                    continue \n",
    "                p = stats.pearsonr(X.T[idx], X.T[idy])      \n",
    "                result.append(f\"{names[idx]:>10} VS {names[idy]:<10} : Statistics = {p[0]:<20} pvalue = {p[1]}\\n\")\n",
    "            \n",
    "        return str().join(result[::-1])\n",
    "\n",
    "    # The method that calculates the Confidence Interval\n",
    "    \n",
    "    def con_int(self): \n",
    "        X, b, n, d, var, rel = self.X, self.b, self.n, self.d, self.var(), self.rel()\n",
    "        # ska vi välja confidence leven baserat på R2 ???\n",
    "        # är α == R2? \n",
    "        # βˆ i ± tα/2σˆ 2√cii\n",
    "        \n",
    "        c = np.linalg.pinv( (X.T @ X) )*var\n",
    "        σ2 = var\n",
    "        α = 1-rel\n",
    "        i = 1\n",
    "        t = n-d-1\n",
    "        #t = stats.t(rel/2, n-d-1)\n",
    "        \n",
    "        \n",
    "        return b[i] + t*(α/2) * σ2 * np.sqrt(c[i,i])\n",
    "        # confidence level sigma \n",
    "        # p_value == nummer av extreme värden?\n",
    "\n",
    "    # The method that calculates the Confidence Interval V2 \n",
    "    \n",
    "    def con_int_2(self):\n",
    "        X, n, var = self.X, self.n, self.var()\n",
    "        #X=X[:,1]\n",
    "        Sxx = (n*np.sum(np.square(X)) - np.square(np.sum(X)))/n\n",
    "        X_mean = X.mean()\n",
    "        se_intercept = var * ((1/n)+np.square(X_mean)/Sxx)\n",
    "        return np.sqrt(se_intercept)\n",
    "    \n",
    "   \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08568360240228182\n",
      "0.10220685815302728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([22.1, 10.4,  9.3, 18.5, 12.9,  7.2, 11.8, 13.2,  4.8, 10.6,  8.6,\n",
       "       17.4,  9.2,  9.7, 19. , 22.4, 12.5, 24.4, 11.3, 14.6, 18. , 12.5,\n",
       "        5.6, 15.5,  9.7, 12. , 15. , 15.9, 18.9, 10.5, 21.4, 11.9,  9.6,\n",
       "       17.4,  9.5, 12.8, 25.4, 14.7, 10.1, 21.5, 16.6, 17.1, 20.7, 12.9,\n",
       "        8.5, 14.9, 10.6, 23.2, 14.8,  9.7, 11.4, 10.7, 22.6, 21.2, 20.2,\n",
       "       23.7,  5.5, 13.2, 23.8, 18.4,  8.1, 24.2, 15.7, 14. , 18. ,  9.3,\n",
       "        9.5, 13.4, 18.9, 22.3, 18.3, 12.4,  8.8, 11. , 17. ,  8.7,  6.9,\n",
       "       14.2,  5.3, 11. , 11.8, 12.3, 11.3, 13.6, 21.7, 15.2, 12. , 16. ,\n",
       "       12.9, 16.7, 11.2,  7.3, 19.4, 22.2, 11.5, 16.9, 11.7, 15.5, 25.4,\n",
       "       17.2, 11.7, 23.8, 14.8, 14.7, 20.7, 19.2,  7.2,  8.7,  5.3, 19.8,\n",
       "       13.4, 21.8, 14.1, 15.9, 14.6, 12.6, 12.2,  9.4, 15.9,  6.6, 15.5,\n",
       "        7. , 11.6, 15.2, 19.7, 10.6,  6.6,  8.8, 24.7,  9.7,  1.6, 12.7,\n",
       "        5.7, 19.6, 10.8, 11.6,  9.5, 20.8,  9.6, 20.7, 10.9, 19.2, 20.1,\n",
       "       10.4, 11.4, 10.3, 13.2, 25.4, 10.9, 10.1, 16.1, 11.6, 16.6, 19. ,\n",
       "       15.6,  3.2, 15.3, 10.1,  7.3, 12.9, 14.4, 13.3, 14.9, 18. , 11.9,\n",
       "       11.9,  8. , 12.2, 17.1, 15. ,  8.4, 14.5,  7.6, 11.7, 11.5, 27. ,\n",
       "       20.2, 11.7, 11.8, 12.6, 10.5, 12.2,  8.7, 26.2, 17.6, 22.6, 10.3,\n",
       "       17.3, 15.9,  6.7, 10.8,  9.9,  5.9, 19.6, 17.3,  7.6,  9.7, 12.8,\n",
       "       25.5, 13.4])"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main.ipynb\n",
    "import pandas as pd\n",
    "path = \"../Resources/\" \n",
    "data_set = pd.read_csv(path+\"Advertising.csv\")\n",
    "data_set.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "lr = LinearRegression(data_set, \"sales\")\n",
    "\n",
    "# testa med e istället f\n",
    "print(lr.con_int())\n",
    "print(lr.con_int_2())\n",
    "lr.Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8688009956311592\n",
      "\n",
      "G\n",
      "1. Number of Features\n",
      "   3\n",
      "\n",
      "2. Sample Size\n",
      "   198\n",
      "\n",
      "3. Variance  \n",
      "   0.006308685487583493\n",
      "\n",
      "4. Standard Deviation \n",
      "    0.07942723391622984\n",
      "\n",
      "5. Significance of the Regression \n",
      "    pvalue: 7.998510997422736e-141\n",
      "\n",
      "6. Relevance of the Regression\n",
      "    0.9971212473220574\n",
      "\n",
      "VG\n",
      "1. Significance tests on Individual Variables\n",
      "    p-value Bias      : 1.3694429113257573e-146\n",
      "      p-value Kinematic : 2.2799778946075336e-236\n",
      "      p-value Geometric : 0.0\n",
      "      p-value Inertial  : 1.9192831125684836e-242\n",
      "      \n",
      "      \n",
      "\n",
      "2. A function or method that calculates the Pearson number\n",
      "      Inertial VS Geometric  : Statistics = 0.9183300308547001   pvalue = 7.951572627158216e-81\n",
      "      Inertial VS Kinematic  : Statistics = 0.9686707504997814   pvalue = 1.588545639896567e-120\n",
      "     Geometric VS Kinematic  : Statistics = 0.8631350761065918   pvalue = 4.5604633624399433e-60\n",
      "    \n",
      "    \n",
      "         \n"
     ]
    }
   ],
   "source": [
    "data_set = pd.read_csv(path+\"Small-diameter-flow.csv\") \n",
    "data_set.drop(columns=[\"Unnamed: 0\",\"Observer\"], inplace=True)\n",
    "data_set\n",
    "lr = LinearRegression(data_set, \"Flow\")\n",
    "print( lr.con_int() )\n",
    "lr.print_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "G\n",
      "1. Number of Features\n",
      "   4\n",
      "\n",
      "2. Sample Size\n",
      "   198\n",
      "\n",
      "3. Variance  \n",
      "   0.006272292538356673\n",
      "\n",
      "4. Standard Deviation \n",
      "    0.07919780639864132\n",
      "\n",
      "5. Significance of the Regression \n",
      "    pvalue: 1.7265182348384377e-139\n",
      "\n",
      "6. Relevance of the Regression\n",
      "    0.9971526073277638\n",
      "\n",
      "VG\n",
      "1. Significance tests on Individual Variables\n",
      "    p-value Bias      : 3.2273690263899853e-147\n",
      "      p-value Kinematic : 5.730580151466907e-236\n",
      "      p-value Geometric : 0.0\n",
      "      p-value Inertial  : 1.1628066959545507e-241\n",
      "      p-value Observer  : 2.3422411107265474e-44\n",
      "      \n",
      "      \n",
      "\n",
      "2. A function or method that calculates the Pearson number\n",
      "      Observer VS Inertial   : Statistics = 0.12198107336291035  pvalue = 0.08690459468332266\n",
      "      Observer VS Geometric  : Statistics = 0.17519913369993184  pvalue = 0.013557203955629581\n",
      "      Observer VS Kinematic  : Statistics = 0.10322658943843983  pvalue = 0.14784118487116096\n",
      "      Inertial VS Geometric  : Statistics = 0.9183300308547001   pvalue = 7.951572627158216e-81\n",
      "      Inertial VS Kinematic  : Statistics = 0.9686707504997814   pvalue = 1.588545639896567e-120\n",
      "     Geometric VS Kinematic  : Statistics = 0.8631350761065918   pvalue = 4.5604633624399433e-60\n",
      "    \n",
      "    \n",
      "         \n"
     ]
    }
   ],
   "source": [
    "data_set = pd.read_csv(path+\"Small-diameter-flow.csv\") \n",
    "data_set.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_set\n",
    "lr = LinearRegression(data_set, \"Flow\")\n",
    "#print( lr.con_int() )\n",
    "lr.print_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
