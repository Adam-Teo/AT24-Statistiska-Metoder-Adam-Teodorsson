{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression.py\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class LinearRegression: \n",
    "    def __init__(self, X, Y, features, response):\n",
    "        self._X = X \n",
    "        self.Y = Y \n",
    "        self.features = features\n",
    "        self.response = response  \n",
    " \n",
    "    @property \n",
    "    def X(self):\n",
    "        X = self._X\n",
    "        X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "        return X\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return self._X.shape[0]\n",
    "    \n",
    "    @property\n",
    "    def d(self):\n",
    "        return self._X.shape[1] \n",
    "     \n",
    "    @property \n",
    "    def b(self):\n",
    "        X = self.X \n",
    "        Y = self.Y\n",
    "        return np.linalg.pinv( X.T @ X) @ X.T @ Y\n",
    "        \n",
    "    # SSE | Residual Sum of Squares | Sum of Squared Error\n",
    "    # The closer this is to zero the more accuret the prediction (in theory)\n",
    "    @property \n",
    "    def SSE(self):\n",
    "        X, Y, b = self.X, self.Y, self.b\n",
    "        return  sum( np.square( Y - X @ b ) )\n",
    "    \n",
    "    # SSR | Explained Sum of Squares | Regression Sum of Squares\n",
    "    @property\n",
    "    def SSR(self):\n",
    "        X, Y_mean, b = self.X, self.Y.mean(), self.b\n",
    "        return sum( np.square( X @ b - Y_mean ) )\n",
    "    \n",
    "    # SST | Syy | Total Sum of Squares\n",
    "    @property \n",
    "    def SST(self):\n",
    "        Y = self.Y\n",
    "        Y_mean = Y.mean() \n",
    "        return sum( np.square(Y-Y_mean) )\n",
    "    \n",
    "    # Picks the confidence level\n",
    "    # If it's below 0.68 it sets it to R2 \n",
    "    @property \n",
    "    def confidence_level(self):\n",
    "        R2 = self.R2()  \n",
    "        confidence_levels = [0.997, 0.95, 0.9, 0.8, 0.68, R2]\n",
    "        return [ cl for cl in confidence_levels if cl <= R2 ][0]\n",
    "\n",
    "    def print_all(self):\n",
    "        all = f\"\"\"\n",
    "G\n",
    "1.     Features: {self.d}\n",
    "2.  Sample Size: {self.n}\n",
    "3.     Variance: {self.var()[0]:.5f}\n",
    "4.  S.Deviation: {self.std()[0]:.5f}\n",
    "5. Significance: {self.sig()[0]}\n",
    "6.    Relevance: {self.R2()[0]:.5f}\n",
    "\n",
    "VG\n",
    "1. Individual Significance\n",
    "{ str().join( f\"{row}\\n{str()}\" for row in self.sig_var().split(\"\\n\") ) }2. Pairs of Pearsons\n",
    "{ str().join( f\"{row}\\n{str()}\" for row in self.pearson().split(\"\\n\") ) }3. Confidence Interval \n",
    "{ str().join( f\"{row}\\n{str()}\" for row in self.con_int().split(\"\\n\") ) }4. Confidence Level: {self.confidence_level}\n",
    "         \"\"\"\n",
    "        print(all)\n",
    "\n",
    "    # The Method that Calculates The Variance | S^ | sigma^2 \n",
    "    # On average how much our predicted responses will diviate from the regression line\n",
    "    def var(self):\n",
    "        SSE, n, d = self.SSE, self.n, self.d \n",
    "        return SSE/(n - d - 1)\n",
    "\n",
    "    # The Method that Calculated The Standard Deviation | S | sigma\n",
    "    # Measure the same thing as the Variances but in a more readable but less fair unit\n",
    "    def std(self):\n",
    "        var = self.var()\n",
    "        return np.sqrt(var)\n",
    "    \n",
    "    # The Method that reports The Significance of the Regression\n",
    "    # The closer the pvalue is to zero the less likely it is that the correlation\n",
    "    # we observe between the features and the respons is coincidental. \n",
    "    # We want the p-value to be less than 0.05 aka 5% to confidently reject the H0 hypothesis\n",
    "    def sig(self): \n",
    "        SSR, d, n, var = self.SSR, self.d, self.n, self.var()\n",
    "        \n",
    "        sig_statistic = (SSR/d)/var\n",
    "     \n",
    "        # Survival Function of the F-Distrubution\n",
    "        p_significance = stats.f.sf(sig_statistic, d, n-d-1)\n",
    "        return p_significance\n",
    "    \n",
    "    # The method that reports The Relevance of Regression\n",
    "    # Reports how big of a range our model can reliably predict. \n",
    "    # So if our R2 value is 0.90 than we could predict 90% of \n",
    "    # all responses within, or relatively close to, the standard deviation\n",
    "    def R2(self):\n",
    "        SSR, SST = self.SSR, self.SST\n",
    "        return SSR/SST\n",
    "\n",
    "\n",
    "    # Significance of the Variables\n",
    "    # This reports the significance each feature have on the model\n",
    "    def sig_var(self):   \n",
    "        X, b, d, n, std, var, features = self.X, self.b, self.d, self.n, self.std(), self.var(), self.features\n",
    "    \n",
    "        # Variance/Covariance Matrix\n",
    "        c = np.linalg.pinv( (X.T @ X) )*var\n",
    "\n",
    "        # Significans Statisitca Array\n",
    "        ssa = [ b[i]/(std * np.sqrt(c[i,i])) for i in range(1, c.shape[1])]\n",
    "        \n",
    "        cdf = stats.t.cdf(ssa, n-d-1)\n",
    "        sf =  stats.t.sf(ssa, n-d-1)\n",
    "        p = [ 2 * min(cdf[idx], sf[idx]) for idx in range(len(ssa)) ]\n",
    "\n",
    "        result = str().join( f\"{features[idx]:<10}: pvalue = {p[idx][0]}\\n\" for idx in range(len(p))  )\n",
    "        return result\n",
    "\n",
    "    # The Method that calculates the Pearson number between all pairs of parameters\n",
    "    # Reports how muc correlation exists between each pair of features\n",
    "    # the closer these values are to zero the less correlation exists between the pair\n",
    "    def pearson(self):\n",
    "        X, features = self.X, self.features\n",
    "        \n",
    "        result = list()\n",
    "        \n",
    "        X = X[:,1:]\n",
    "        for idx in range(len(features)):\n",
    "            for idy in range(idx):\n",
    "                if idy == idx:\n",
    "                    continue \n",
    "                p = stats.pearsonr(X.T[idx], X.T[idy])    \n",
    "                result.append(f\"{features[idx]:<9} VS {features[idy]:<9} : {p[0]:.10f}\\n\")\n",
    "        \n",
    "        return str().join(result[::-1])\n",
    "    \n",
    "    # The method that calculates the Confidence Interval\n",
    "    def con_int(self):\n",
    "        X,  b, n, d, var, std, features = self.X, self.b, self.n, self.d, self.var(), self.std(), self.features\n",
    "      \n",
    "        a = 1-self.confidence_level \n",
    "        df = n-d-1\n",
    "        results = list()\n",
    "\n",
    "        # Variance/Covariance Matrix\n",
    "        c = np.linalg.pinv( (X.T @ X) )*var\n",
    "        features.insert(0, \"bias\")\n",
    "        for i in range(d+1):           \n",
    "            ci = (b[i], stats.t.ppf(a/2, df) * std * np.sqrt(c[i][i]))\n",
    "\n",
    "            # Returns the result in the order of low to high\n",
    "            low_to_high = min((ci[0][0]-ci[1][0]), (ci[0][0]+ci[1][0])),max((ci[0][0]-ci[1][0]),(ci[0][0]+ci[1][0]))\n",
    "            result = f\"{features[i-1]}: {ci[0][0]:.5f} ± {abs(ci[1][0]):.5f} | interval:[{low_to_high[0]:.5f} <> {low_to_high[1]:.5f}]\\n\"\n",
    "            results.append(result)\n",
    "        \n",
    "        return str().join(results)   \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"../Resources/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "G\n",
      "1.     Features: 3\n",
      "2.  Sample Size: 198\n",
      "3.     Variance: 0.00631\n",
      "4.  S.Diviation: 0.07943\n",
      "5. Significance: 3.8879169683951874e-246\n",
      "6.    Relevance: 0.99712\n",
      "\n",
      "VG\n",
      "1. Individual Significance\n",
      "Kinematic : pvalue = 2.2799778946075336e-236\n",
      "Geometric : pvalue = 0.0\n",
      "Inertial  : pvalue = 1.9192831125684836e-242\n",
      "\n",
      "2. Pairs of Pearsons\n",
      "Inertial  VS Geometric : 0.9183300309\n",
      "Inertial  VS Kinematic : 0.9686707505\n",
      "Geometric VS Kinematic : 0.8631350761\n",
      "\n",
      "3. Confidence Interval \n",
      "Inertial: -2.55979 ± 0.10089 | interval:[-2.66069 <> -2.45890]\n",
      "bias: 0.86872 ± 0.01163 | interval:[0.85709 <> 0.88034]\n",
      "Kinematic: 3.61042 ± 0.00785 | interval:[3.60257 <> 3.61827]\n",
      "Geometric: -0.75369 ± 0.00938 | interval:[-0.76307 <> -0.74430]\n",
      "\n",
      "4. Confidence Level: 0.997\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "# An instance without the Observer\n",
    "data_set = pd.read_csv(path+\"Small-diameter-flow.csv\") \n",
    "X = data_set.drop(columns=[\"Unnamed: 0\", \"Observer\", \"Flow\"])\n",
    "Y = data_set[[\"Flow\"]]\n",
    "flow = LinearRegression(X.values, Y.values, list(X.columns), Y.columns[0])\n",
    "\n",
    "flow.print_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kinematic : pvalue = 5.730580151466907e-236\n",
      "Geometric : pvalue = 0.0\n",
      "Inertial  : pvalue = 1.1628066959545507e-241\n",
      "Observer  : pvalue = 2.3422411107265474e-44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An instance with the Observer\n",
    "data_set = pd.read_csv(path+\"Small-diameter-flow.csv\") \n",
    "X = data_set.drop(columns=[\"Unnamed: 0\", \"Flow\"])\n",
    "Y = data_set[[\"Flow\"]]\n",
    "flow_obs = LinearRegression(X.values, Y.values, list(X.columns), Y.columns[0])\n",
    "\n",
    "#flow_obs.print_all()\n",
    "print(flow_obs.sig_var())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Discussion\n",
    "Question: ”Is there an observer bias in the data collected for the small-diameter flow measurements?\"\n",
    "\n",
    "Answer: For there to be an Observer Bias the categorical Observer feature\\\n",
    "needs to be significant.\\\n",
    "To prove significance we need to reject the H0 hypothesis.\\\n",
    "To confidently reject the H0 hypothesis we need to see a P-Value off less\\\n",
    "then 0.05 meaning that there would be less then 5% chance that the\\\n",
    "correlation between the Observer and Flow would be coincidental.\n",
    "\n",
    "We can check the p-value between the Observer and Flow using the\\\n",
    "sig_var method, as was done in the cell above.\\\n",
    "This gives us a p-value for the Observer of 2.3422411107265474e-44,\\\n",
    "which is way less then 0.05 which means that we can, with confidence,\\\n",
    "reject the H0 hypothesis, the Observer feature is significant, the\\\n",
    "correlation is not accidental and therefore there is most likely an Obserber Bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
